
/* Listing 5-1 */

val flight_summary = spark.read.format("csv").option("header", "true") .option("inferSchema","true").load(“<path>/chapter5/data/flights/flight-summary.csv")

flight_summary.count

flight_summary.printSchema

/* Listing 5-2 */

flight_summary.select(count("origin_airport"), count("dest_airport").as("dest_count")).show


/* Listing 5-3 */
import org.apache.spark.sql.Row
case class Movie(actor_name:String, movie_title:String, produced_year:Long)
val badMoviesDF = Seq( Movie(null, null, 2018L), 
                       Movie("John Doe", "Awesome Movie", 2018L), 
                       Movie(null, "Awesome Movie", 2018L), 
                       Movie("Mary Jane", "Awesome Movie", 2018L)).toDF

badMoviesDF.show                       


badMoviesDF.select(count("actor_name"), count("movie_title"), count("produced_year"), count("*")).show


/* Listing 5-4 */
// countDistinct
flight_summary.select(countDistinct("origin_airport"), countDistinct("dest_airport"), count("*")).show

/* Listing 5-5 */
flight_summary.select(count("count"),countDistinct("count"), approx_count_distinct("count", 0.05)).show

flight_summary.select(countDistinct("count")).show

flight_summary.select(approx_count_distinct("count", 0.01)).show

/* Listing 5-6 */
flight_summary.select(min("count"), max("count")).show

/* Listing 5-7 */
flight_summary.select(sum("count")).show

/* Listing 5-8 */
flight_summary.select(sumDistinct("count")).show

/* Listing 5-9 */
flight_summary.select(avg("count"), (sum("count") / count("count"))).show

/* Listing 5-10 */
flight_summary.select(skewness("count"), kurtosis("count")).show

/* Listing 5-11 */
flight_summary.select(variance("count"), var_pop("count"), stddev("count"), stddev_pop("count")).show

/* Listing 5-12 */
flight_summary.groupBy("origin_airport").count().show(5, false)

/* Listing 5-13 */
// group by multiple columns
flight_summary.groupBy('origin_state, 'origin_city).count.where('origin_state === "CA").orderBy('count.desc).show(5)

/* Listing 5-14 */
import org.apache.spark.sql.functions._
flight_summary.groupBy("origin_airport").agg(count("count").as("count"), min("count"), max("count"), sum("count")).show(5)

/* Listing 5-15 */
flight_summary.groupBy("origin_airport").agg("count" -> "count", "count" -> "min", "count" -> "max", "count" -> "sum").show(5)

/* Listing 5-16 */
val highCountDestCities = flight_summary.where('count > 5500).groupBy("origin_state").agg(collect_list("dest_city").as("dest_cities"))

highCountDestCities.withColumn("dest_city_cnt", size('dest_cities)).show(5, false)


/* Listing 5-17 */
import org.apache.spark.sql.Row
case class Student(name:String, gender:String, weight:Int, graduation_year:Int)
val studentsDF = Seq(Student("John", "M", 180, 2015), 
                         Student("Mary", "F", 110, 2015), 
                         Student("Derek", "M", 200, 2015),
                         Student("Julie", "F", 109, 2015), 
                         Student("Allison", "F", 105, 2015),
                         Student("kirby", "F", 115, 2016), 
                         Student("Jeff", "M", 195, 2016)).toDF

studentsDF.groupBy("graduation_year").pivot("gender").avg("weight").show()

/* Listing 5-18 */
studentsDF.groupBy("graduation_year").pivot("gender").agg(min("weight").as("min"),max("weight").as("max"),avg("weight").as("avg")).show()


/* Listing 5-19 */
studentsDF.groupBy("graduation_year").pivot("gender", Seq("M")).agg(min("weight").as("min"),max("weight").as("max"),avg("weight").as("avg")).show()


/* Listing 5-20 */
case class Employee(first_name:String, dept_no:Long)
val employeeDF = Seq( Employee("John", 31), 
                      Employee("Jeff", 33), 
                      Employee("Mary", 33), 
                      Employee("Mandy", 34),
                      Employee("Julie", 34),
                      Employee("Kurt", null.asInstanceOf[Int])
                     ).toDF

case class Dept(id:Long, name:String)
val deptDF = Seq( Dept(31, "Sales"), 
                      Dept(33, "Engineering"), 
                      Dept(34, "Finance"), 
                      Dept(35, "Marketing")
                     ).toDF

// register as views
employeeDF.createOrReplaceTempView("employees")
deptDF.createOrReplaceTempView("departments")

/* Listing 5-21 */
val joinExpression = employeeDF.col("dept_no") === deptDF.col("id")

employeeDF.join(deptDF, joinExpression, "inner").show

employeeDF.join(deptDF, joinExpression).show

spark.sql("select * from employees JOIN departments on dept_no == id").show

/* Listing 5-22 */

employeeDF.join(deptDF, 'dept_no === 'id).show

employeeDF.join(deptDF, employeeDF.col("dept_no") === deptDF.col("id")).show

employeeDF.join(deptDF).where('dept_no === 'id).show


/* Listing 5-23 */
employeeDF.join(deptDF, 'dept_no === 'id, "left_outer").show

spark.sql("select * from employees LEFT OUTER JOIN departments on dept_no == id").show

/* Listing 5-24 */
employeeDF.join(deptDF, 'dept_no === 'id, "right_outer").show

spark.sql("select * from employees RIGHT OUTER JOIN departments on dept_no == id").show

/* Listing 5-25 */
employeeDF.join(deptDF, 'dept_no === 'id, "outer").show

spark.sql("select * from employees FULL OUTER JOIN departments on dept_no == id").show

/* Listing 5-26 */
employeeDF.join(deptDF, 'dept_no === 'id, "left_anti").show

spark.sql("select * from employees LEFT ANTI JOIN departments on dept_no == id").show

/* Listing 5-27 */
employeeDF.join(deptDF, 'dept_no === 'id, "left_semi").show

spark.sql("select * from employees LEFT SEMI JOIN departments on dept_no == id").show

/* Listing 5-28 */
employeeDF.crossJoin(deptDF).count

spark.sql("select * from employees CROSS JOIN departments").show(30)

/* Listing 5-29 */

val deptDF2 = deptDF.withColumn("dept_no", 'id)
deptDF2.printSchema

val dupNameDF = employeeDF.join(deptDF2, employeeDF.col("dept_no") === deptDF2.col("dept_no"))
dupNameDF.printSchema

/* Listing 5-30 */
dupNameDF.select("dept_no")


/* Listing 5-31 */
dupNameDF.select(deptDF2.col("dept_no"))

/* Listing 5-32 */
val noDupNameDF = employeeDF.join(deptDF2, "dept_no")
noDupNameDF.printSchema


/* Listing 5-33 */
import org.apache.spark.sql.functions.broadcast
employeeDF.join(broadcast(deptDF), employeeDF.col("dept_no") === deptDF.col("id")).show


/* Listing 5-34 */
val testDateTSDF = Seq((1, "2018-01-01", "2018-01-01 15:04:58:865", "01-01-2018", "12-05-2017 45:50")).toDF("id", "date", "timestamp", "date_str", "ts_str")

val testDateResultDF = testDateTSDF.select(to_date('date).as("date1"), to_timestamp('timestamp).as("ts1"), to_date('date_str, "MM-dd-yyyy").as("date2"), to_timestamp('ts_str, "MM-dd-yyyy mm:ss").as("ts2"), unix_timestamp('timestamp).as("unix_ts"))

testDateResultDF.printSchema
testDateResultDF.show

/* Listing 5-35 */
testDateResultDF.select(date_format('date1, "dd-MM-YYYY").as("date_str"),date_format('ts1, "dd-MM-YYYY HH:mm:ss").as("ts_str"), from_unixtime('unix_ts,"dd-MM-YYYY HH:mm:ss").as("unix_ts_str")).show

/* Listing 5-36 */
val employeeData = Seq( 
                 ("John", "2016-01-01", "2017-10-15"), 
                 ("May", "2017-02-06", "2017-12-25")).toDF("name", "join_date", "leave_date")

employeeData.show

employeeData.select('name, datediff('leave_date, 'join_date).as("days"), months_between('leave_date, 'join_date).as("months"), last_day('leave_date).as("last_day_of_mon")).show

val oneDate = Seq(("2018-01-01")).toDF("new_year")

oneDate.select(date_add('new_year, 14).as("mid_month"), date_sub('new_year, 1).as("new_year_eve"), next_day('new_year, "Mon").as("next_mon")).show

/* Listing 5-37 */
val valentimeDateDF = Seq(("2018-02-14 05:35:55")).toDF("date")
valentimeDateDF.select(year('date).as("year"), quarter('date).as("quarter"),month('date).as("month"), weekofyear('date).as("woy"),dayofmonth('date).as("dom"),dayofyear('date).as("doy"),hour('date).as("hour"), minute('date).as("minute"), second('date).as("second")).show

val dateTimeDF = Seq( 
                 (1, "2018-01-01", "2018-01-01 15:04:58.865"), 
                 (2, "2017-02-06", "2017-02-06 17:24:18.326"),    
                 (3, "2015-06-19", "2016-06-19 02:10:45.561")).toDF("id","joinDate", "txDate")



dateDF.select($"id",               
              to_date($"joinDate").as("joinDate"),               
              quarter($"joinDate").as("quarter"),              
              dayofyear($"joinDate").as("doy"),              
              weekofyear($"joinDate").as("woy"),              
              last_day($"joinDate").as("lastday_month"),
              $"txDate",
              datediff(current_timestamp(), $"txDate").as("num_day_ago")).show

/* Listing 5-38 */
val sparkDF = Seq(("  Spark  ")).toDF("name")
sparkDF.select(trim('name).as("trim"), ltrim('name).as("ltrim"), rtrim('name).as("rtrim")).show

sparkDF.select(trim('name).as("trim")).select(lpad('trim, 8, "-").as("lpad"), rpad('trim, 8, "=").as("rpad")).show

val sparkAwesomeDF = Seq(("Spark", "is", "awesome")).toDF("subject", "verb", "adj")

sparkAwesomeDF.select(concat_ws(" ",'subject, 'verb, 'adj).as("sentence")).select(lower('sentence).as("lower"), upper('sentence).as("upper"), initcap('sentence).as("initcap"), reverse('sentence).as("reverse")).show

sparkAwesomeDF.select('subject, translate('subject, "ar", "oc").as("translate")).show

/* Listing 5-39 */
val rhymeDF = Seq(("A fox saw a crow sitting on a tree singing \"Caw! Caw! Caw!\"")).toDF("rhyme")

rhymeDF.select(regexp_replace('rhyme, "fox|crow", "animal").as("new_rhyme")).show(false)

/* Listing 5-40 */
val rhymeDF = Seq(("A fox saw a crow sitting on a tree singing \"Caw! Caw! Caw!\"")).toDF("rhyme")

rhymeDF.select(regexp_replace('rhyme, "[a-z]*o[xw]", "animal").as("new_rhyme")).show(false)

rhymeDF.select(regexp_extract('rhyme, "[a-z]*o[xw]",0).as("substring")).show

/* Listing 5-41 */
val numberDF = Seq((3.14159, 4.8, 2018).toDF("pie", "gpa", year")

numberDF.select(round('pie).as("pie0"), round('pie, 1).as("pie1"), round('pie, 2).as("pie2"), round('gpa).as("gpa"), round('year).as("year")).show

/* Listing 5-42 */

val tasksDF = Seq(("Monday", Array("Pick Up John", "Buy Milk", "Pay Bill"))).toDF("day", "tasks")
tasksDF.printSchema

tasksDF.select('day, size('tasks).as("size"), sort_array('tasks).as("sorted_tasks"), array_contains('tasks, "Pay Bill").as("shouldPayBill")).show(false)

tasksDF.select('day, explode('tasks)).show


/* Listing 5-43 */
import org.apache.spark.sql.types._


val todos = """{"day": "Monday","tasks": ["Pick Up John","Buy Milk","Pay Bill"]}"""
val todoStrDF = Seq((todos)).toDF("todos_str")

todoStrDF.printSchema


val todoSchema = new StructType().add("day", StringType).add("tasks",  ArrayType(StringType))


val todosDF = todoStrDF.select(from_json('todos_str, todoSchema).as("todos"))
todosDF.printSchema

todosDF.select('todos.getItem("day"), 'todos.getItem("tasks"), 'todos.getItem("tasks").getItem(0).as("first_task")).show

todosDF.select('todos.getItem("day"), explode('todos.getItem("tasks"))).show

// to json
todosDF.select(to_json('todos)).show(false)

/* Listing 5-44 */
val numDF = spark.range(1,11,1,5)
numDF.rdd.getNumPartitions

numDF.select('id, monotonically_increasing_id().as("m_ii"), spark_partition_id().as("partition")).show

/* Listing 5-45 */
val dayOfWeekDF = spark.range(1,8,1)
dayOfWeekDF.show

dayOfWeekDF.select('id, when('id === 1, "Mon").when('id === 2, "Tue").when('id === 3, "Wed").when('id === 4, "Thu").when('id === 5, "Fri").when('id === 6, "Sat").when('id === 7, "Sun").as("dow")).show

dayOfWeekDF.select('id, when('id === 6, "Weekend").when('id === 7, "Weekend").otherwise("Weekday").as("day_type")).show

/* Listing 5-46 */
case class Movie(actor_name:String, movie_title:String, produced_year:Long)
val badMoviesDF = Seq( Movie(null, null, 2018L), 
                       Movie("John Doe", "Awesome Movie", 2018L)).toDF

badMoviesDF.select(coalesce('actor_name, lit("no_name")).as("new_title")).show


/* Listing 5-47 */

import org.apache.spark.sql.functions.udf


case class Student(name:String, score:Int)
val studentDF = Seq(Student("Joe", 85), 
               Student("Jane", 90), 
               Student("Mary", 55)).toDF()

studentDF.createOrReplaceTempView("students")

def letterGrade(score:Int) : String = {   
   score match {    
     case score if score > 100 => "Cheating"    
     case score if score >= 90 => "A"    
     case score if score >= 80 => "B"    
     case score if score >= 70 => "C"    
     case _ => "F"  
   } 
}

val letterGradeUDF = udf(letterGrade(_:Int):String)

studentDF.select($"name",$"score",
                 letterGradeUDF($"score").as("grade")).show

spark.sqlContext.udf.register("letterGrade", 
                              letterGrade(_: Int): String)

spark.sql("select name, score, letterGrade(score) as grade from students").show

/* Listing 5-48 */

// rollups and cubes
val flight_summary = spark.read.format("csv") .option("header", "true") .option("inferSchema","true").load("<path>/chapter5/data/flights/flight-summary.csv")

val twoStatesSummary = flight_summary.select('origin_state, 'origin_city, 'count).where('origin_state === "CA" || 'origin_state === "NY").where('count > 1 && 'count < 20).where('origin_city =!= "White Plains").where('origin_city =!= "Newburgh").where('origin_city =!= "Mammoth Lakes").where('origin_city =!= "Ontario")

twoStatesSummary.show


twoStatesSummary.rollup('origin_state, 'origin_city).agg(sum("count") as "total").orderBy('origin_state.asc_nulls_last, 'origin_city.asc_nulls_last).show

/* Listing 5-49 */
twoStatesSummary.cube('origin_state, 'origin_city).agg(sum("count") as "total").orderBy('origin_state.asc_nulls_last, 'origin_city.asc_nulls_last).show



/* Listing 5-50 */
val appleOneYearDF = spark.read.format("csv") .option("header", "true") .option("inferSchema","true") .load("<path>/chapter5/data/stocks/aapl-2017.csv")

appleOneYearDF.printSchema

val appleWeeklyAvgDF = appleOneYearDF.groupBy(window('Date, "1 week")).agg(avg("Close").as("weekly_avg"))

appleWeeklyAvgDF.printSchema

appleWeeklyAvgDF.orderBy("window.start").selectExpr("window.start", "window.end", "round(weekly_avg, 2) as weekly_avg").show(5)


/* Listing 5-51 */
val appleMonthlyAvgDF = appleOneYearDF.groupBy(window('Date, "4 week", "1 week")).agg(avg("Close").as("monthly_avg"))

appleMonthlyAvgDF.orderBy("window.start").selectExpr("window.start", "window.end", "round(monthly_avg, 2) as monthly_avg").show(5)


/* Listing 5-52 */
import org.apache.spark.sql.expressions.Window

val txDataDF = Seq(("John", "2017-07-02", 13.35), ("John", "2017-07-06", 27.33), ("John", "2017-07-04", 21.72), ("Mary",  "2017-07-07", 69.74), ("Mary",  "2017-07-01", 59.44), ("Mary",  "2017-07-05", 80.14)).toDF("name", "tx_date", "amount")


val forRankingWindow = Window.partitionBy("name").orderBy(desc("amount"))

val txDataWithRankDF = txDataDF.withColumn("rank", rank().over(forRankingWindow))

txDataWithRankDF.where('rank < 3).show(10)


/* Listing 5-53 */
val forEntireRangeWindow = Window.partitionBy("name").orderBy(desc("amount")).rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)

val amountDifference = max(txDataDF("amount")).over(forEntireRangeWindow) - txDataDF("amount")

val txDiffWithHighestDF = txDataDF.withColumn("amount_diff", round(amountDifference, 3))

txDiffWithHighestDF.show

/* Listing 5-54 */
val forMovingAvgWindow = Window.partitionBy("name").orderBy("tx_date").rowsBetween(Window.currentRow-1,Window.currentRow+1)

val txMovingAvgDF = txDataDF.withColumn("moving_avg", round(avg("amount").over(forMovingAvgWindow), 2))
txMovingAvgDF.show

/* Listing 5-55 */
val forCulmulativeSumWindow = Window.partitionBy("name").orderBy("tx_date").rowsBetween(Window.unboundedPreceding,Window.currentRow)

val txCulmulativeSumDF = txDataDF.withColumn("culm_sum", round(sum("amount").over(forCulmulativeSumWindow),2))
txCulmulativeSumDF.show

/* Listing 5-56 */
txDataDF.createOrReplaceTempView("tx_data")

// top 2 transaction amount
spark.sql("select name, tx_date, amount, rank from (select name, tx_date, amount, RANK() OVER (PARTITION BY name ORDER BY amount DESC) as rank from tx_data) where rank < 3").show

// max transaction diff
spark.sql("select name, tx_date, amount, round((max_amount - amount),2) as amount_diff from (select name, tx_date, amount, MAX(amount) OVER (PARTITION BY name ORDER BY amount DESC RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as max_amount from tx_data)").show

// moving average
spark.sql("select name, tx_date, amount, round(moving_avg,2) as moving_avg from (select name, tx_date, amount, AVG(amount) OVER (PARTITION BY name ORDER BY tx_date ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as moving_avg from tx_data)").show

// culmulative sum
spark.sql("select name, tx_date, amount, round(culm_sum,2) as moving_avg from (select name, tx_date, amount, SUM(amount) OVER (PARTITION BY name ORDER BY tx_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as culm_sum from tx_data)").show

/* Listing 5-57 */

val moviesDF = spark.read.load(”<path>/chapter4/data/movies/movies.parquet")

val newMoviesDF = moviesDF.filter('produced_year > 1970).withColumn("produced_decade", 'produced_year + 'produced_year % 10).select('movie_title, 'produced_decade).where('produced_decade > 2010)

newMoviesDF.explain(true)

/* Listing 5-58 */
spark.range(1000).filter("id > 100").selectExpr("sum(id)").explain()

